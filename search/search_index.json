{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>This project uses LTSP, Proxmox, LDAP and Apache Guacamole to deploy a fully functioning training center webapp, where users/students can access a linux OS through the convenience of their web browser.</p> <p>The administrator can manage users through LDAP including creation of users, setting password, disabling as well as allocating a VM to a user. Using LDAP means the authentication for WebApp login as well as the OS can be managed from a single place</p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#recap","title":"Recap","text":"<p>We have set up Proxmox on a server, configured LDAP on a virtual machine, installed LTSP on a separate machine, and deployed Apache Guacamole as Docker containers on a virtual machine.</p>"},{"location":"configuration/#setting-up-wake-on-lan-for-vms","title":"Setting Up Wake On LAN for VMs","text":"<p>Info</p> <p>The following scripts and commands are to be run on the Proxmox server.</p> <p>Once our platform is deployed and users begin utilizing the virtual machines, a challenge arises when a user shuts down a VM. They won't have the ability to start it back up on their own. Fortunately, Apache Guacamole allows us to send Wake-On-LAN (WOL) packets to power on the VM before attempting a connection. However, since we're working with virtual machines instead of physical ones, the network interface card (NIC) isn't actively listening for WOL packets once the VM is shut down. To resolve this, we'll configure the Proxmox server to listen for WOL packets and start the appropriate VM based on the packet's target MAC address.</p> <p>The following Script was sourced from proxmox forums with slight modifications. (credit goes to EpicLPer) Original forum post link <pre><code>#!/bin/bash\n\nIFACENAME=\"vmbr1\"\n\nwhile true; do\n  sleep 2\n  wake_mac=$(tcpdump -c 1 -UlnXi \"$IFACENAME\" ether proto 0x0842 or udp port 9 2&gt;/dev/null |\\\n  sed -nE 's/^.*20:  (ffff|.... ....) (..)(..) (..)(..) (..)(..).*$/\\2:\\3:\\4:\\5:\\6:\\7/p')\n  echo \"Captured magic packet for address: \\\"${wake_mac}\\\"\"\n  echo -n \"Looking for existing VM: \"\n  matches=($(grep -il ${wake_mac} /etc/pve/qemu-server/*))\n  if [[ ${#matches[*]} -eq 0 ]]; then\n    echo \"${#matches[*]} found\"\n  echo -n \"Looking for existing LXC: \"\n  matches=($(grep -il ${wake_mac} /etc/pve/lxc/*))\n  if [[ ${#matches[*]} -eq 0 ]]; then\n    echo \"${#matches[*]} found\"\n    continue\n  elif [[ ${#matches[*]} -gt 1 ]]; then\n    echo \"${#matches[*]} found, using first found\"\n  else\n    echo \"${#matches[*]} found\"\n  fi\n  vm_file=$(basename ${matches[0]})\n  vm_id=${vm_file%.*}\n  details=$(pct status ${vm_id} -verbose | egrep \"^name|^status\")\n  name=$(echo ${details} | awk '{print $2}')\n  status=$(echo ${details} | awk '{print $4}')\n  if [[ \"${status}\" != \"stopped\" ]]; then\n    echo \"SKIPPED CONTAINER ${vm_id} : ${name} is ${status}\"\n  else\n    echo \"STARTING CONTAINER ${vm_id} : ${name} is ${status}\"\n    pct start ${vm_id}\n  fi\n    continue\n  elif [[ ${#matches[*]} -gt 1 ]]; then\n    echo \"${#matches[*]} found, using first found\"\n  else\n    echo \"${#matches[*]} found\"\n  fi\n  vm_file=$(basename ${matches[0]})\n  vm_id=${vm_file%.*}\n  details=$(qm status ${vm_id} -verbose | egrep \"^name|^status\")\n  name=$(echo ${details} | awk '{print $2}')\n  status=$(echo ${details} | awk '{print $4}')\n  if [[ \"${status}\" != \"stopped\" ]]; then\n    echo \"SKIPPED VM ${vm_id} : ${name} is ${status}\"\n  else\n    echo \"STARTING VM ${vm_id} : ${name} is ${status}\"\n    qm start ${vm_id}\n  fi\ndone\n</code></pre></p> <p>Replace <code>IFACENAME</code> with the name of the network interface that Proxmox will be listening on. To test this setup, save the file on the proxmox server and execute it manually (it will eventually be set up as a systemd service, but for now, we'll leave it as is). </p> <p>Next, create a new virtual machine or use an existing one, power it off, and retrieve its MAC address from the Proxmox web UI. Then, from another machine with <code>wol</code> installed, run the following command to send a Wake-On-LAN (WOL) packet:</p> <p><pre><code># Replace with the target MAC address\nwol --port 9 AA:BB:CC:DD:EE:FF\n</code></pre> If the VM powers on, your setup is successful. If not, ensure that the machine sending the WOL packet can reach the Proxmox server and verify that no firewalls are blocking port 9 between them.</p>"},{"location":"configuration/#systemd-service","title":"Systemd service","text":"<p>Now that the script is working like indented, we can set it up as a systemd service to ensure it runs on boot and restarts if it crashes. Create a new service file with the following content:</p> <p><pre><code>[Unit]\nDescription=Wake-on-LAN for Proxmox Virtual Environments\nAfter=network.target\n\n[Service]\nType=simple\nRestart=always\nUser=root\nExecStart=/usr/local/bin/scripts/wol-vms.sh\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> Make sure to place the script in the correct directory on the proxmox server and update the <code>ExecStart</code> path accordingly. Save the file as <code>wol-vms.service</code> in <code>/etc/systemd/system/</code> and run the following commands to enable and start the service:</p> <p><pre><code>systemctl enable --now wol-vms\n</code></pre> Now lets move on to creating some VMs for our users to access.</p>"},{"location":"configuration/#creating-virtual-machines","title":"Creating Virtual Machines","text":"<p>We will create a few virtual machines for our users to access. run the following command on the proxmox server to create 5 virtual machines:</p> <p><pre><code>for i in {1..5}; do qm create 200$i --name VM-200$i --cpu x86-64-v2-AES --cores 2 --sockets 1 --memory 4096 --net0 virtio,bridge=vmbr1,firewall=1 --ostype l26 --scsihw virtio-scsi-single; done\n</code></pre> This command will create 5 virtual machines with 2 cores, 4GB of RAM, and a single network interface connected to <code>vmbr1</code>. The VMs will be named <code>VM-2001</code>, <code>VM-2002</code>, <code>VM-2003</code>, <code>VM-2004</code>, and <code>VM-2005</code>. Feel free to adjust the resources and names as needed. You can find more info on the qm command here.</p> <p>We now have 2 more steps to complete before our platform is ready for users to access. We need to expose the VNC output from proxmox, so that guacamole can connect to it and we need to add the VMs to LDAP and associate them with users. Let's start with the first step.</p>"},{"location":"configuration/#exposing-vnc-output","title":"Exposing VNC Output","text":"<p>By default, Proxmox does not expose the VNC output of virtual machines. To enable this, we need to add a few lines to the VM configuration file. Run the following command to add the necessary lines to the configuration files:</p> <p><pre><code>for i in {1..5}; do echo \"args: -vnc 192.168.111.123:$((i + 77))\" &gt;&gt; /etc/pve/qemu-server/200$i.conf; done\n</code></pre> This command will add the <code>-vnc</code> argument to the VM configuration files, exposing the VNC output on the IP address of the Proxmox server and port <code>5900+xx</code>, where <code>xx</code> is the VM number added to 77. For example, <code>VM-2001</code> will be exposed on <code>192.168.111.123:5978</code>. There is a dedicated wiki page on the Proxmox website that explains the VNC options in more detail here.</p> <p>Make sure to replace the IP address with the one that your Proxmox server is using. Now that the VNC output is exposed, we can move on to the final step.</p>"},{"location":"configuration/#adding-vms-to-ldap","title":"Adding VMs to LDAP","text":"<p>Info</p> <p>The following scripts and commands are to be run on the LDAP server.</p> <p>To allow users to access the VMs, we need to add them to LDAP and associate them with the users. We will use the <code>ldapadd</code> command to add the VMs to LDAP. Create a new file called <code>vms.ldif</code> on the LDAP server with the following content:</p> <p><pre><code># VM 1\ndn: cn=VM 1,ou=groups,dc=example,dc=com\nobjectClass: guacConfigGroup\nobjectClass: groupOfNames\nguacConfigProtocol: vnc\n\nguacConfigParameter: hostname=192.168.111.123 # Replace with the IP address of the Proxmox server where VNCs are exposed\nguacConfigParameter: port=5978 # Replace with the port number of the VNC output\nguacConfigParameter: wol-send-packet=true\nguacConfigParameter: wol-mac-addr=AA:BB:CC:DD:EE:FF # Replace with the MAC address of the VM\nguacConfigParameter: wol-broadcast-addr=192.168.111.123 # Replace with the proxmox server IP where our wol-vms script is listening\nguacConfigParameter: wol-udp-port=9\nguacConfigParameter: wol-wait-time=5 # Time to wait for the VM to start before connecting\nmember: uid=ldapadmin,ou=people,dc=example,dc=com # Replace with the user that should have access to the VM. You can add multiple users by repeating this line\n</code></pre> Replace the values in the <code>vms.ldif</code> file with the appropriate values for your setup. Duplicate the entry for each VM you created, changing the <code>cn=</code>, <code>port=</code>, <code>wol-mac-addr=</code> and <code>member=</code> values accordingly. Once the file is ready, run the following command to add the VMs to LDAP:</p> <p><pre><code>ldapadd -x -D cn=admin,dc=example,dc=com -W -f vms.ldif\n</code></pre> You will be prompted to enter the LDAP admin password. You can verify that the VMs were added successfully by running the following command:</p> <p><pre><code>ldapsearch -x -LLL -b ou=groups,dc=example,dc=com\n</code></pre> This command will list all the groups in the <code>ou=groups,dc=example,dc=com</code> branch. You should see the VMs listed with the appropriate configuration. If everything looks good, you can now access the VMs using Apache Guacamole.</p>"},{"location":"configuration/#wrapping-up","title":"Wrapping Up","text":"<p>We have successfully set up Wake-On-LAN for VMs on the Proxmox server, created virtual machines for users to access, exposed the VNC output, and added the VMs to LDAP. Users can now access the VMs using Apache Guacamole. Spin up a browser and navigate to the Guacamole web interface to test the connection. If everything is working as expected, you have successfully set up a remote desktop platform using Proxmox, LDAP, LTSP, and Apache Guacamole. Congratulations!.</p>"},{"location":"configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Creating and managing LTSP Images</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#proxmox-and-ldap-base-installation","title":"Proxmox and LDAP Base Installation","text":"<p>Follow the base installation guide for the following before proceeding:</p> <ul> <li>Proxmox Virtual Environment Installation</li> <li>LDAP Setup on Debian</li> <li>LTSP Installation (Server side only)</li> </ul>"},{"location":"installation/#configuring-ltsp-server-to-use-ldap","title":"Configuring LTSP server to use LDAP","text":"<p>Since all of our VMs will mount the home directory from the LTSP server, we need to configure the LTSP server to authenticate users from the LDAP server. Run the following commands to install and configure LDAP client on the LTSP server:</p> <p><pre><code>sudo apt install libpam-ldapd\n</code></pre> You will be prompted to enter the LDAP server URI, base DN, etc on a TUI prompt. Enter the details and when you reach the following page, select the following options:</p> <ol> <li>passwd</li> <li>group</li> <li>shadow</li> </ol> <p></p> <p>Warning</p> <p>Make sure to select the correct options, as selecting the wrong options can cause the server to not authenticate against the LDAP server.</p> <p>Now for the final step, we can configure the VM to create a home directory for the user if it doesn't exist. Run the following command:</p> <p><pre><code>sudo pam-auth-update\n</code></pre> Select the option \"Create home directory on login\" and click \"Ok\" like so:</p> <p></p> <p>Note</p> <p>Without this option, the server will not create a home directory for the user if it doesn't exist and the user will get dropped to <code>/</code> when they log in.</p>"},{"location":"installation/#ldap-configuration-for-guacamole","title":"LDAP Configuration for Guacamole","text":"<p>Once LDAP is set up, you need to add a custom schema to store virtual machine information, which Guacamole will use to locate available VMs.</p> <ol> <li> <p>Head over to the Apache Guacamole downloads page and find the <code>guacamole-auth-ldap-x.x.x.tar.gz</code> file. The latest version is <code>1.5.5</code> at the time of writing, but download the most recent version available.</p> </li> <li> <p>Extract the tarball. Inside the <code>schema</code> folder, you'll find a file named <code>guacConfigGroup.ldif</code>.</p> </li> <li> <p>Apply this schema to your LDAP instance with the following command:</p> <pre><code>ldapadd -Q -Y EXTERNAL -H ldapi:/// -f schema/guacConfigGroup.ldif\n</code></pre> <p>You may be prompted to enter the LDAP admin password. Upon success, you should see an output like this:</p> <pre><code>adding new entry \"cn=guacConfigGroup,cn=schema,cn=config\"\n</code></pre> </li> </ol> <p>Now that the schema is in place, you're ready to begin adding virtual machines to LDAP. You can do this either manually using LDIF files or via the Apache Directory Studio. This will be covered in the Management section.</p>"},{"location":"installation/#installing-guacamole","title":"Installing Guacamole","text":"<p>To deploy Guacamole using Docker, you first need to prepare the necessary database tables.</p>"},{"location":"installation/#setting-up-database-tables","title":"Setting Up Database Tables","text":"<p>Run the following command to launch a temporary Guacamole container that generates SQL scripts for table creation:</p> <pre><code>docker run --rm guacamole/guacamole /opt/guacamole/bin/initdb.sh --postgresql &gt; initdb.sql\n</code></pre>"},{"location":"installation/#docker-compose-setup-for-guacamole","title":"Docker Compose Setup for Guacamole","text":"<ol> <li> <p>Create a <code>compose.yml</code> file for the deployment. Start by defining the PostgreSQL database service:</p> <pre><code>services:\n  db:\n    container_name: guacamoledb\n    image: postgres:alpine\n    restart: unless-stopped\n    logging:\n        driver: \"local\"\n    environment:\n      POSTGRES_PASSWORD: 'dontusethispassword'\n      POSTGRES_USER: 'guacuser'\n    volumes:\n      - pgdata:/var/lib/postgresql/data\nvolumes:\n  pgdata:\n</code></pre> </li> <li> <p>Replace the placeholder password with a secure one, then save the file in a folder and run:</p> <pre><code>docker compose up -d\n</code></pre> <p>This will start the PostgreSQL container.</p> </li> <li> <p>Copy the generated <code>initdb.sql</code> file to the PostgreSQL container:</p> <pre><code>docker cp initdb.sql guacamoledb:/initdb.sql\n</code></pre> </li> <li> <p>Execute the SQL script inside the container to set up the necessary database tables:</p> <pre><code>docker exec guacamoledb psql -U guacuser -f /initdb.sql\n</code></pre> </li> </ol> <p>Make sure the container name and username match those defined in your <code>compose.yml</code> file.</p>"},{"location":"installation/#completing-the-guacamole-setup","title":"Completing the Guacamole Setup","text":"<p>Stop the currently running database container to add the rest of the configuration:</p> <p><pre><code>docker compose down\n</code></pre> Next, append the following services to your existing <code>compose.yml</code> file:</p> <p><pre><code>  guacd:\n    container_name: guacd\n    image: guacamole/guacd\n    restart: unless-stopped\n    logging:\n      driver: \"local\"\n\n  guacamole:\n    container_name: guacamole\n    image: guacamole/guacamole\n    restart: unless-stopped\n    logging:\n      driver: \"local\"\n    ports:\n      - 8080:8080\n    volumes:\n      - ./guacamole-home:/guacamole-home\n    environment:\n      # Set Apache Tomcat to use / as the Guacamole web context\n      WEBAPP_CONTEXT: \"ROOT\"\n\n      # Directory for custom extensions, duplicated to the container home\n      GUACAMOLE_HOME: \"/guacamole-home\"\n\n      GUACD_HOSTNAME: \"guacd\"\n      POSTGRESQL_HOSTNAME: \"guacamoledb\"\n      POSTGRESQL_DATABASE: \"guacuser\"\n      POSTGRESQL_USER: \"guacuser\"\n      POSTGRESQL_PASSWORD: \"dontusethispassword\"\n      POSTGRESQL_AUTO_CREATE_ACCOUNTS: \"true\"\n\n      # LDAP configuration (adjust to match your setup)\n      LDAP_HOSTNAME: \"191.168.111.222\"\n      LDAP_PORT: 389\n      LDAP_USER_BASE_DN: \"ou=people,dc=example,dc=com\" # Users available for login\n      LDAP_CONFIG_BASE_DN: \"ou=groups,dc=example,dc=com\" # Virtual machines available for use\n\n      # Enable 2FA (if desired)\n      # TOTP_ENABLED: \"true\"\n    depends_on:\n      - guacdb\n      - guacd\n</code></pre> The final <code>compose.yml</code> file should resemble the following:</p> <p><pre><code>services:\n  db:\n    container_name: guacamoledb\n    image: postgres:alpine\n    restart: unless-stopped\n    logging:\n        driver: \"local\"\n    environment:\n      POSTGRES_PASSWORD: 'dontusethispassword'\n      POSTGRES_USER: 'guacuser'\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n  guacd:\n    container_name: guacd\n    image: guacamole/guacd\n    restart: unless-stopped\n    logging:\n      driver: \"local\"\n\n  guacamole:\n    container_name: guacamole\n    image: guacamole/guacamole\n    restart: unless-stopped\n    logging:\n      driver: \"local\"\n    ports:\n      - 8090:8080\n    volumes:\n      - ./guacamole-home:/guacamole-home\n    environment:\n      WEBAPP_CONTEXT: \"ROOT\"\n      GUACAMOLE_HOME: \"/guacamole-home\"\n      GUACD_HOSTNAME: \"guacd\"\n      POSTGRESQL_HOSTNAME: \"guacamoledb\"\n      POSTGRESQL_DATABASE: \"guacuser\"\n      POSTGRESQL_USER: \"guacuser\"\n      POSTGRESQL_PASSWORD: \"dontusethispassword\"\n      POSTGRESQL_AUTO_CREATE_ACCOUNTS: \"true\"\n      LDAP_HOSTNAME: \"191.168.111.222\"\n      LDAP_PORT: 389\n      LDAP_USER_BASE_DN: \"ou=people,dc=example,dc=com\"\n      LDAP_CONFIG_BASE_DN: \"ou=groups,dc=example,dc=com\"\n    depends_on:\n      - guacdb\n      - guacd\nvolumes:\n  pgdata:\n</code></pre> Now you can start the Guacamole services:</p> <p><pre><code>docker compose up -d\n</code></pre> You will now be able to access Guacamole at <code>http://your-server-ip:8080</code>. If you have a domain name, you can set up a reverse proxy to point to the Guacamole service.</p>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>Head over to Configuration to finalize some more configurations before we can use the system</p>"},{"location":"management/","title":"Management","text":"<p>Now that we have the platform ready to use, the only thing missing is some LTSP images which can be used by our newly created VMs to boot off of.</p>"},{"location":"management/#creating-ltsp-images","title":"Creating LTSP Images","text":"<p>LTSP provides 3 ways to create images:</p> <ol> <li>Chrootless: use the server root (/) as the template for the clients</li> <li>Raw virtual machine image: graphically maintain e.g. a VirtualBox VM.</li> <li>Chroot: manually maintain a chroot directory using console commands.</li> </ol> <p>We will be going with the second option, as it is the most straightforward and easy to maintain.</p>"},{"location":"management/#installing-virtualbox","title":"Installing VirtualBox","text":"<p>Info</p> <p>The following instructions are supposed to be executed on the LTSP server.</p> <p>First, we need to install VirtualBox. You can download the latest version from the official website. cd to the directory where the downloaded file is located and run the following command:</p> <pre><code>sudo apt install ./&lt;downloaded_file&gt;.deb\n</code></pre> <p>Note</p> <ul> <li>Make sure to add your user to the <code>vboxusers</code> group by running the following command and logging out and back in: <pre><code>sudo usermod -aG vboxusers $USER\n</code></pre></li> <li>Make sure you have virtualization enabled in your BIOS settings.</li> </ul> <p>Once installed, open it up. You should see a window like this:</p> <p></p> <p>Tip</p> <p>If you were prompted to choose between expert mode and basic mode, choose expert mode.</p> <p>Click on the \"New\" button to create a new VM. You should see a window like this:</p> <p> Customize it to your liking and expand the \"Hard disk\" section. LTSP require VMDK images with preallocated space, so make sure to select \"VMDK\" and \"Pre-allocate Full Size\" as shown below:</p> <p></p> <p>Click on \"Create\" and you should see a window like this:</p> <p></p> <p>Start up the VM and go through the installation process. Once you are done, you should have a VM ready to be used as an LTSP image. You may also install any additional software you want to be included in the image.</p>"},{"location":"management/#installing-ltsp-client","title":"Installing LTSP Client","text":"<p>Now that we have a VM ready, we need to install LTSP on it. Boot up the VM and run the following commands:</p> <p><pre><code>sudo apt update\nsudo add-apt-repository ppa:ltsp\nsudo apt update\nsudo apt install --install-recommends ltsp epoptes-client\n</code></pre> This is standard LTSP client installation. You can find more information on how to configure LTSP here.</p>"},{"location":"management/#installing-and-configuring-ldap","title":"Installing and Configuring LDAP","text":"<p>Since we are using LDAP to authenticate users when they log in to the VM, we need to install and configure LDAP on the VM also. Follow the same steps as you did on the LTSP server to install and configure LDAP on the VM, but you may skip the <code>pam-auth-update</code> step as it is not required on the client.</p>"},{"location":"management/#exporting-the-vm-as-an-ltsp-image","title":"Exporting the VM as an LTSP Image","text":"<p>Once you are done with the configuration, shut down the VM and export it as an LTSP image. Run the following command to export the VM:</p> <p><pre><code>sudo ln -s \"/home/user/VirtualBox VMs/ubuntu/ubuntu-flat.vmdk\" /srv/ltsp/ubuntu.img\nltsp image ubuntu\n</code></pre> Replace <code>/home/user/VirtualBox VMs/ubuntu/ubuntu-flat.vmdk</code> with the path to your VM's VMDK file. The second command creates an LTSP image from the VMDK file.</p> <p>Note</p> <p>Note that we are using the <code>xx-flat.vmdk</code> file instead of the regular one. this is because LTSP requires the actual disk image with pre-allocated space to create an LTSP image. The <code>flat</code> in the filename indicates that the disk is pre-allocated.</p> <p>It should take a couple of minutes to complete the process. Follow the guide over at LTSP - Maintaining client images for more information on how to create LTSP images.</p>"},{"location":"management/#adding-the-ltsp-image-to-the-ltsp-boot-menu","title":"Adding the LTSP Image to the LTSP boot menu","text":"<p>After you create your initial image, or if you ever create additional images, run the following command to generate an iPXE menu and copy the iPXE binaries in TFTP:</p> <pre><code>ltsp ipxe\n</code></pre> <p>Once done, all of the proxmox VMs are ready to boot off of the LTSP server. Launch apache guacamole on a browser and login with the credentials you set up earlier. You should see the VMs listed on the left side of the screen or go straight to VNC depending on the number of VMs assgined to the user. Click on any of them to start the VM. You should see the VM booting off of the LTSP server.</p> <p></p> <p>That's it! You now have a working LTSP server with LDAP and Guacamole integration. You can create more LTSP images and go through the same process to add them to the server.</p>"},{"location":"management/#next-steps","title":"Next Steps","text":"<ul> <li>Quirks and workarounds</li> </ul>"},{"location":"prerequisites/","title":"Prerequisites","text":"<p>To ensure smooth system operation, it is important to meet specific hardware and software requirements. Adhering to these guidelines will help avoid potential issues.</p>"},{"location":"prerequisites/#hardware-requirements","title":"Hardware Requirements","text":"<p>The Guacamole server and LDAP server can be hosted on virtual machines within a Proxmox environment for better management. However, the LTSP server must be a physical machine due to its need for running VirtualBox to create LTSP client images. Although nested virtualization allows for LTSP on a virtual machine, we will use a dedicated physical server for this purpose.</p>"},{"location":"prerequisites/#virtual-machine-server","title":"Virtual Machine Server","text":"<p>The more powerful your server, the more virtual machines (VMs) it can host. When serving a specific operating system like Ubuntu, you should follow its official system requirements. According to the Ubuntu 24.04 download page, the recommended specifications are 4GB of RAM and a 2GHz CPU. For this project, Ubuntu 24.04 VMs have been tested with 1 CPU core from an 80-core server-grade CPU (Intel Xeon Silver 4316 @ 2.30GHz) and 2GB of RAM per VM. This configuration works well for basic tasks like web browsing and light programming.</p>"},{"location":"prerequisites/#ltsp-server","title":"LTSP Server","text":"<p>The LTSP server is another key hardware requirement. All VMs will boot from this server and mount user home directories using SSHFS/NFS. Both the CPU and network should be powerful enough to handle these tasks. The Debian Wiki provides a detailed overview.</p> <p>Tldr</p> <ul> <li>Gigabit networking is required between the LTSP server and the switch. 100 Mbps is acceptable for clients connecting to the switch, though Gigabit is recommended.</li> <li>2GB of RAM plus an additional 30MB per client is needed.</li> <li>CPU requirements are calculated in terms of benchmark scores. In testing, a 4-core desktop CPU (Intel Core i5-7500 @ 3.40GHz) handled 50 clients with SSHFS for home directory mounting.</li> </ul>"},{"location":"prerequisites/#guacamole-server","title":"Guacamole Server","text":"<p>Apache Guacamole streams virtual machine display outputs to web browsers and supports various remote desktop protocols like VNC and RDP. We will use the VNC protocol, as Proxmox can be configured to share a VM\u2019s display output beyond its web UI.</p> <p>Since Guacamole handles video encoding for multiple clients, a solid configuration would involve 4 cores and 8GB of RAM for every 100 VMs. All necessary programs for Guacamole will be deployed using Docker.</p> <p>Tip</p> <p>The Guacamole server can run as a VM inside the Proxmox environment.</p>"},{"location":"prerequisites/#ldap-server","title":"LDAP Server","text":"<p>The LDAP server has minimal hardware requirements, as it mainly handles authentication and directory services. A single-core processor and 1GB of RAM should be sufficient for this VM. Although LDAP can be deployed as a Docker container alongside Guacamole, it's preferable to use separate VMs to avoid a single point of failure. If resources permit, use separate virtual machines for Guacamole and LDAP.</p>"},{"location":"prerequisites/#software-requirements","title":"Software Requirements","text":""},{"location":"prerequisites/#ltsp","title":"LTSP","text":"<p>LTSP officially supports Debian-based distributions and recommends Debian. Installation instructions are available on the official LTSP documentation site.</p>"},{"location":"prerequisites/#ldap","title":"LDAP","text":"<p>For Debian and Debian-based distributions, the OpenLDAP package is called slapd. Alternatively, any LDAP3-compatible software can be used.</p>"},{"location":"prerequisites/#guacamole","title":"Guacamole","text":"<p>Guacamole will be installed as Docker containers, which include three images:</p> <ul> <li>Guacamole Daemon</li> <li>Guacamole Webserver</li> <li>A database (PostgreSQL recommended)</li> </ul>"},{"location":"prerequisites/#other-tools","title":"Other Tools","text":"<ul> <li>Docker: For managing Guacamole Docker containers.</li> <li>VirtualBox: To create custom Linux installations to be exported as LTSP images.</li> <li>Apache Directory Studio: LDAP Management and Overview.</li> </ul>"},{"location":"quirks/","title":"Quirks","text":"<p>This page contains some quirks that you may encounter while setting up the platform and how to work around them.</p> <p>Note</p> <p>Any changes done to the LTSP images will require you to rebuild the image. This can be done by running the following commands: <pre><code>sudo ltsp image &lt;image_name&gt;\n</code></pre></p>"},{"location":"quirks/#disable-auto-suspend","title":"Disable Auto suspend","text":"<p>By default in gnome the client image is set to suspend after 5 minutes of inactivity depending on the client distro defaults. This can be troublesome since it usually results in a frozen system and no way for user to restart it. To disable this system wide, you'll have to edit the following files  <code>/etc/dconf/db/local.d/00-power</code> <pre><code># Specify the dconf path\n[org/gnome/settings-daemon/plugins/power]\n\n# Enable screen dimming\nidle-dim=false\n\n# Set brightness after dimming\nidle-brightness=30\n</code></pre> <code>/etc/dconf/db/local.d/00-screensaver</code> <pre><code># Specify the dconf path\n[org/gnome/desktop/session]\n\n# Number of seconds of inactivity before the screen goes blank\n# Set to 0 seconds if you want to deactivate the screensaver.\nidle-delay=uint32 0\n\n# Specify the dconf path\n[org/gnome/desktop/screensaver]\n\n# Number of seconds after the screen is blank before locking the screen\nlock-delay=uint32 0\n</code></pre></p>"},{"location":"quirks/#kali-linux-does-not-authenticate-ldap-users","title":"Kali Linux does not authenticate LDAP users","text":"<p>You may encounter an issue where Kali Linux clients are unable to authenticate LDAP users even after configuring LDAP on Kali VM in virtualbox. This is due to the fact that in Kali, the <code>nslcd.service</code> not enabled by default. To fix this, you'll have to manually enable the service after installing LDAP packages. Run the following command: <pre><code>sudo systemctl enable --now nslcd.service\n</code></pre></p>"},{"location":"quirks/#username-prompt-on-login","title":"Username prompt on login","text":"<p>Since all users exists on the LDAP server, GDM will not show the list of users on the login screen and the user should click on the \"Not listed?\" button to enter the username manually. This can be annoying for users who are not aware of this. To disable user list and show username prompt on login, you'll have to edit <code>/etc/gdm3/greeter.dconf-defaults</code> to uncomment the <code>disable-user-list=true</code> line.</p>"},{"location":"quirks/#using-local-disk-instead-of-ltsp-image","title":"Using local disk instead of LTSP image","text":"<p>If you want to use a local disk instead of the VMs booting off of the LTSP server, you can do by attaching a disk of required size into every VM and changing the boot order. Doing this manually is time consuming, so you may use the following commands to automate the process:</p>"},{"location":"quirks/#attaching-disks","title":"Attaching disks","text":"<pre><code>for i in {1..10}; do qm set $i --scsi1 &lt;storage_name&gt;:&lt;disk_size&gt;; done\n# To attach a disk of size 32GB from a ZFS pool called Tank to VMs 1 to 10 as scsi1, use the following command:\n# for i in {1..10}; do qm set $i --scsi1 Tank:32; done\n</code></pre>"},{"location":"quirks/#mounting-isos","title":"Mounting ISOs","text":"<pre><code>for i in {1..10}; do qm set $i --ide0 &lt;iso_path&gt;; done\n# To mount an ISO located at /zfsimages/templates/iso/ubuntu.iso to VMs 1 to 10 as ide0, use the following command:\n# for i in {1..10}; do qm set $i --ide0 /zfsimages/templates/iso/ubuntu.iso; done\n</code></pre>"},{"location":"quirks/#setting-boot-order","title":"Setting boot order","text":"<p>You need to change the boot order of the VM to boot off of the ISO if you are planning to install an OS on it. By default, all the VMs created using the wizard will boot off of the LTSP server. <pre><code>for i in {1..10}; do qm set $i --boot 'order=scsi0;ide0'; done\n# To set the boot order to scsi0 and ide0 for VMs 1 to 10, use the following command:\n# for i in {1..10}; do qm set $i --boot 'order=scsi0;ide0'; done\n</code></pre></p>"},{"location":"usage/","title":"Usage","text":"<p>Now that all of our services are up and running, we can setup our Backend and Frontend for creating virtual machines.</p>"},{"location":"usage/#installing-management-webapp","title":"Installing Management Webapp","text":"<p>Now that we have the LTSP server ready to use, we need to install the management web application. The application frontend is written in react and the backend is written using fastAPI. If you haven't already, clone the repository at GitHub.</p> <pre><code>git clone https://github.com/pzerone/WebVirt\n</code></pre>"},{"location":"usage/#backend-setup","title":"Backend setup","text":"<p>Warning</p> <p>It is mandatory that the backend is hosted on the proxmox server itself. This is so that the backend can access the proxmox configuration files directly and manipulate them.</p> <p>We will copy just the backend to the proxmox server. The backend is written in python and uses FastAPI. We will use uvicorn to run the backend.</p> <pre><code>cd WebVirt\nscp -r backend root@&lt;proxmox_ip&gt;:  # Require SSH server inside proxox host\n</code></pre> <p>Now on the proxmox host, run the following to install python3 venv and install the dependencies:</p> <pre><code>apt install python3-venv\ncd backend\npython3 -m venv .env\nsource .env/bin/activate\npip install -r requirements.txt\n</code></pre> <p>Now run the following command to start the backend:</p> <p><pre><code>uvicorn main:app --host 0.0.0.0 --port 8000\n</code></pre> This will start the backend FastAPI server on port <code>8000</code>. Check if the backend is running by going to http://:8000/ping. You should see a response like this: <pre><code>{\"ping\":\"pong\"}\n</code></pre>"},{"location":"usage/#frontend-setup","title":"Frontend setup","text":"<p>Now that our backend is up, we can setup our frontend. You have the freedom to run the frontend anywhere you want. Just make sure it can access the backend.</p>"},{"location":"usage/#building-the-frontend","title":"Building the frontend","text":"<p>You need npm installed to build the frontend.</p> <p>Note</p> <p>Make sure to set the backend URL in the frontend <code>.env</code> file to <code>http://&lt;proxmox_ip&gt;:8000</code> before building</p> <pre><code>cd WebVirt/frontend\nnpm install\nnpm run build\n</code></pre> <p>copy the dist folder and all its contents to a webserver like nginx or apache. The login screen should look like this:</p> <p></p> <p>Try logging in as the LDAP admin user we created earlier and you should be greeted by the following page.</p> <p></p>"},{"location":"usage/#creating-virtual-machines","title":"Creating virtual machines.","text":"<p>Once we have the user list, we can use this web app to automatically create all the users and virtual machines for them in our proxmox server. To do this, get the user list in a csv file in the following format</p> <p><code>first_name, last_name</code></p> <p>Usernames and passwords are auto generated by the backend. Once the CSV file is ready, upload it through this page. It should give you a preview of the user list if the csv is in valid format.</p> <p></p> <p>Fill up the rest of the form and click upload. You will now see a response page with the created users with their username and password. You can download the file by clicking on the download button at the bottom.</p> <p>Note</p> <p>The virtual machines and the users are being created in the background. It takes approximately 5 seconds per user to fully create the user and virtual machine. Do not attempt to login until the users are created. You can view the progress of the VMs being created in the proxmox web interface.</p> <p>Share the login credentials from the response CSV file to the users via preferred means.</p> <p>Warning</p> <p>Make sure you set the duration to be long enough for the users to complete the course. The Virtual machines will be auto deleted after this time expires.</p>"},{"location":"usage/#next-steps","title":"Next Steps","text":"<p>Quirks and workarounds</p>"}]}